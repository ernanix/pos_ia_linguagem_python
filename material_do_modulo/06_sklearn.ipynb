{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geographic-nowhere",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Aprendizado de Máquina com SciKit-Learn\"\n",
    "subtitle: \"Linguagem de Programação Aplicada\"\n",
    "author: \"Prof. Alex Kutzke\"\n",
    "date: 24 de abril 2021\n",
    "geometry: margin=2cm\n",
    "output: \n",
    "  ioslides_presentation:\n",
    "    widescreen: true\n",
    "---\n",
    "\n",
    "# SciKit-Learn\n",
    "\n",
    "## O que é o SciKit-Learn\n",
    "\n",
    "- Aprendizado de máquina para Python:\n",
    "  - Ferramentas simples e eficientes para Mineração e Análise de Dados;\n",
    "  - Acessível para todos, e reusável em vários contextos;\n",
    "  - Construído sobre `NumPy`, `SciPy` e `matplotlib`;\n",
    "  - Código aberto, usável comercialmente - Licença BSD;\n",
    "\n",
    "- Biblioteca estável e bem estabelecida:\n",
    "  - Aceita pela comunidade científica.\n",
    "\n",
    "# Aprendizado de Máquina (Machine Learning)\n",
    "\n",
    "## O  Problema da aprendizagem\n",
    "\n",
    "* Considera um conjunto de $`n`$ amostras de dado e então tenta prever propriedades de\n",
    "  dados desconhecidos;\n",
    "* Se cada amostra é mais do que um simples número, ou seja, uma entrada\n",
    "  multidimensional, dizemos que temos vários atributos ou **features**;\n",
    "\n",
    "## Categorias de Aprendizado\n",
    "\n",
    "- Aprendizado Supervisionado:\n",
    "  - Quando os dados possuem os atributos que desejamos prever;\n",
    "  - Nesse caso, o problema pode ser:\n",
    "    - **Classificação**;\n",
    "    - **Regressão**;\n",
    "\n",
    "## Aprendizado Supervisionado - Classificação\n",
    "\n",
    "- Amostras de dados de duas ou mais classes;\n",
    "- Queremos aprender a partir de dados já classificados;\n",
    "- Classificar amostras futuras desconhecidas;\n",
    "- Exemplo:\n",
    "  - Reconhecer dígitos numéricos escritos à mão:\n",
    "    - Algoritmo é treinado a partir de uma série de imagens de dígitos classificados\n",
    "      com o número que representam;\n",
    "\n",
    "## Aprendizado Supervisionado - Regressão\n",
    "\n",
    "- É quando o valor a ser predito consiste de uma ou mais variáveis contínuas (não apenas classes);\n",
    "- Exemplo:\n",
    "  - Prever o tamanho de Salmões a partir da sua idade e peso.\n",
    "\n",
    "## Aprendizado Não Supervisionado\n",
    "\n",
    "- O Aprendizado não supervisionado é aquele em que o conjunto de dados não inclui o \n",
    "  valor que queremos prever;\n",
    "- O objetivo de problemas como este seria a descoberta de grupos de valores similares;\n",
    "  - Clusterização;\n",
    "\n",
    "## Conjuntos de Treinamento e Testes\n",
    "\n",
    "- Aprendizado de máquina está relacionado com o aprendizado sobre dados para aplicação\n",
    "  em dados novos;\n",
    "- É muito comum, nesse meio, realizar a divisão dos dados para a avaliação dos algoritmos:\n",
    "  - **Conjunto de Treino**: dados utilizados para a aprendizagem do modelo;\n",
    "  - **Conjunto de Testes**: dados utilizados para verificar se o modelo apresenta resultados esperados.\n",
    "\n",
    "## Utilizando o SciKit-Learn\n",
    "\n",
    "- A biblioteca `sklearn` é composta por vários itens, dentre eles:\n",
    "  - `sklearn.datasets`;\n",
    "  - `sklearn.feature_extraction`;\n",
    "  - `sklearn.metrics`;\n",
    "  - `sklearn.naive_bayes`;\n",
    "  - `sklearn.neural_network`;\n",
    "  - ...\n",
    "- Mais aqui: https://sklearn.org/modules/classes.html\n",
    "\n",
    "## Utilizando o SciKit-Learn\n",
    "\n",
    "- Portanto, a biblioteca tem uma aparato gigantesco (submódulos) para atender qualquer\n",
    "  necessidade de Aprendizado de Máquina;\n",
    "- Entretanto, isso significa que é necessário conhecer tudo isso;\n",
    "- Conhecer \"por demanda\" é uma boa saída;\n",
    "- Para facilitar, alguns padrões entre os submódulos são seguidos:\n",
    "  - API transformer, por exemplo;\n",
    "\n",
    "## Utilizando o SciKit-Learn\n",
    "\n",
    "- Em geral, costumas-se carregar apenas os módulos separados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-extent",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "- Para facilitar o trabalho com Aprendizado de Máquina, o `sklearn` possuí o submódulo\n",
    "  `datasets`;\n",
    "- Composto por uma série de bases dados para testes algoritmos:\n",
    "  - Dígitos escritos à mão;\n",
    "  - Dados sobre a flor Iris para identificação de suas espécies;\n",
    "  - Dentre vários outros.\n",
    "\n",
    "## Carregando um dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-state",
   "metadata": {},
   "source": [
    "Verifique os itens `.data` e `.target` dos datasets.\n",
    "\n",
    "## Formato do vetor de features\n",
    "\n",
    "- Os classificadores do `sklearn` sempre esperam que as features sejam passadas\n",
    "  em um vetor unidimensional;\n",
    "- Verifique a diferença entre `digits.data` e `digits.image`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-territory",
   "metadata": {},
   "source": [
    "## Aprendizado e predição\n",
    "\n",
    "- No scikit-learn um \"estimador\" (*estimator*) é um objeto Python que implementa os\n",
    "  métodos:\n",
    "  - `fit(X,y)` - treina o modelo;\n",
    "  - `predict(T)` - calcula predições;\n",
    "\n",
    "## Exemplo de Estimador\n",
    "\n",
    "- `sklearn.svm.SVC` implementa **Support Vector Classification**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-tourist",
   "metadata": {},
   "source": [
    "- Mais adiante falaremos sobre os parâmetros do classificador.\n",
    "\n",
    "## Classificando o dataset de dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(digits.data[:-1], digits.target[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(digits.data[-1:])\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-girlfriend",
   "metadata": {},
   "source": [
    "# Exemplos\n",
    "\n",
    "## Reconhecimento de Dígitos\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py\n",
    "\n",
    "Ou\n",
    "\n",
    "[Código no repositório](codes/sklearn/digits/plot-digits-classification.py)\n",
    "\n",
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-treaty",
   "metadata": {},
   "source": [
    "## Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-prevention",
   "metadata": {},
   "source": [
    "## Exibindo 4 imagens do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-defeat",
   "metadata": {},
   "source": [
    "## Reshaping e classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-favor",
   "metadata": {},
   "source": [
    "## Fitting (treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-tonight",
   "metadata": {},
   "source": [
    "Separando a outra metade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict the value of the digit on the second half:\n",
    "expected = digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-albuquerque",
   "metadata": {},
   "source": [
    "## Obtendo métrica\n",
    "\n",
    "- Submódulo `sklearn.metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-liberty",
   "metadata": {},
   "source": [
    "## Exibindo imagens e suas predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-valley",
   "metadata": {},
   "source": [
    "## Classificação de Iris\n",
    "\n",
    "- Tente realizar a mesma classificação, mas agora com o dataset `iris`:\n",
    "  - Deu certo?\n",
    "  - Se sim, tente outro classificador como o `sklearn.naive_bayes.GaussianNB`.\n",
    "\n",
    "## Trabalhando com Texto\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "ou\n",
    "\n",
    "[Código no repositório](code/sklearn/20newsgroup/exemplo.py)\n",
    "\n",
    "## Carregando o dataset 20 newsgroup\n",
    "\n",
    "- Dataset composto por várias mensagens de grupos de notícia classificadas por tema;\n",
    "- Carregaremos apenas 4 temas;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, \n",
    "                                  shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-significance",
   "metadata": {},
   "source": [
    "- Possui o mesmo formato de outros datasets já vistos.\n",
    "\n",
    "## Carregando o dataset 20 newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names\n",
    "\n",
    "twenty_train.target[:10]\n",
    "\n",
    "for t in twenty_train.target[:10]:\n",
    "    print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-houston",
   "metadata": {},
   "source": [
    "## Extraindo features\n",
    "\n",
    "- Na aula passada, extraímos features de mensagens de email:\n",
    "  - Verificamos a presença de palavras nos assuntos das mensagens;\n",
    "- Uma outra forma de retirar features de textos é a contagem de palavras (*bag os words*);\n",
    "\n",
    "## Bag of words\n",
    "  - A forma mais simples de produzir um *bag of words*:\n",
    "    - Atribuir um inteiro `j` para cada palavra $`w_{j}`$ encontrada entre os documentos;\n",
    "    - Para cada documento `i` contar as ocorrências de $`w_{j}`$ e armazenar em `X[i,j]`;\n",
    "\n",
    "  - Algum problema?\n",
    "    - Espaço na memória para **100 mil palavras distintas** (valor típico):\n",
    "      - Cerca de **4GB** para **10 mil documentos** (4 bytes por `float32`);\n",
    "      - Mas a maioria dos dados é 0;\n",
    "  - Solução:\n",
    "    - Matriz esparsa;\n",
    "    - SciKit-learn já possui implementação dessa estrutura;\n",
    "\n",
    "## Tokens com scikit-learn\n",
    "\n",
    "- `CountVectorizer` realiza pré-processamento, *tokenizing*, filtragem de *stopwords* e contagem de palavras de uma só vez:\n",
    "  - Utiliza estrutura `scipy.sparse` para armazenar dados;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts.shape\n",
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-technology",
   "metadata": {},
   "source": [
    "## Transformer API \n",
    "\n",
    "- Reparou o método `fit_transform`?\n",
    "- Ele faz parte de um padrão do scikit-learn, conhecido como *Transformer API*;\n",
    "  - Todos os Objetos que implementam essa API possuem os métodos fit e transform:\n",
    "    - `fit`: realiza a absorção dos dados para um modelo;\n",
    "    - `transform`: realiza transformações necessárias sobre os dados e mantém essas informações salvas para aplicação da mesma transformação em outros conjuntos de dados;\n",
    "    - `fit_transform`: chamada de `fit` e `transform` em sequência;\n",
    "- Em alguns objetos o método `fit` não faz nada. Existe apenas para manter o padrão da API.\n",
    "\n",
    "## De ocorrências para frequências\n",
    "\n",
    "- A simples quantidade de ocorrências de uma palavra pode levar a conclusões incorretas sobre um texto:\n",
    "  - Por que?\n",
    "...\n",
    "- Documentos maiores possuem mais palavras.\n",
    "\n",
    "## De ocorrências para frequências\n",
    "\n",
    "- Para evitar problemas como esse, podemos utilizar duas métricas comuns na área de Recuperação de Informação (*Information Retrieval*):\n",
    "  - Term Frequency (tf): divisão do número de ocorrências pelo total de palavras no documento;\n",
    "  - Term Frequecy/Inverse Document Frequency (td-idf): diminui o peso de palavras que aparecem em muitos documentos;\n",
    "- Para isso, podemos utilizar outro transformador do `sklearn`:\n",
    "\n",
    "## TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-interstate",
   "metadata": {},
   "source": [
    "Ou apenas (agora com idf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-undergraduate",
   "metadata": {},
   "source": [
    "## Treinando um Classificador\n",
    "\n",
    "- Utilizaremos, em um primeiro momento, um classificador da família `naive_bayes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-glass",
   "metadata": {},
   "source": [
    "## Prevendo temas de textos\n",
    "\n",
    "- Os dados de teste devem receber o mesmo processamento de contagem de palavras:\n",
    "  - Daí a vantagem da Tranformer API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-longitude",
   "metadata": {},
   "source": [
    "## Criando um Pipeline\n",
    "\n",
    "- O `sklearn` possui uma forma de facilitar a execução de todas essas etapas (extração\n",
    "  de dados, filtragem, treinamento, ...):\n",
    "  - O **Pipeline**;\n",
    "- Entenda um Pipeline como uma sequencia de objetos python que serão chamados para\n",
    "  realizar uma tarefa;\n",
    "- Pipelines também implementam a Transformer API:\n",
    "  - Ou seja, possuem os métodos `fit`, `transform` e, se for um Estimator,  `predict`;\n",
    "  \n",
    "## Exemplo Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "text_clf.predict(docs_new) # dados de teste podem ser passados sem processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-realtor",
   "metadata": {},
   "source": [
    "## Avaliando o desempenho\n",
    "\n",
    "- Existem diferentes formas de avaliar o desempenho de um classificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                                 shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-loading",
   "metadata": {},
   "source": [
    "## Utilizando outro classificador\n",
    "\n",
    "- Troquemos o naive bayes por support vector machine:\n",
    "  - Qual se sai melhor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "predicted = text_clf.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-substitute",
   "metadata": {},
   "source": [
    "## Outra forma de avaliar performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n",
    "    \n",
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-boost",
   "metadata": {},
   "source": [
    "## Ajustando parâmetros automagicamente\n",
    "\n",
    "- Beleza! Mas e os parâmetros do algoritmo? Caem do céu?\n",
    "  - Na maioria dos casos \"sim\";\n",
    "  - Valores *default* são bastante razoáveis na maioria do tempo;\n",
    "- Por outro lado, conhecimento do problema e do algoritmo, pode dar algumas pistas\n",
    "  sobre ajustes nos parâmentros;\n",
    "  \n",
    "- Entretanto, há outra saída: força bruta; :)\n",
    "\n",
    "## GridSearchCV\n",
    "\n",
    "- Em resumo, basta elencar os parâmetros a serem testados e aguardar o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-disorder",
   "metadata": {},
   "source": [
    "## Utilizando o GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "\n",
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n",
    "\n",
    "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf.best_score_\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-recruitment",
   "metadata": {},
   "source": [
    "## Detalhes do GridSearchCV\n",
    "\n",
    "- O \"CV\" vem de \"cross validation\":\n",
    "  - Divisão dos dados em partes iguais (folds) e validação cruzada entre todas as possibilidades:\n",
    "\n",
    "<div class=\"centered\">\n",
    "![](img/cv.jpg){ width=60% }\n",
    "</div>\n",
    "\n",
    "## Detalhes do GridSearchCV\n",
    "\n",
    "- Podemos definir a quantidade de folds facilmente com o `GridSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-butler",
   "metadata": {},
   "source": [
    "## Padronização\n",
    "\n",
    "- Por vezes, é necessário padronizar os dados utilizados (ou *Standardization*):\n",
    "  - Ou seja, subtrair a média de cada feature e então dividir pelo desvio padrão;\n",
    "- O submódulo `sklearn.preprocessing.StandardScaler` faz isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-extraction",
   "metadata": {},
   "source": [
    "- Existem pacotes para outros tratamentos mais robustos como normalização.\n",
    "\n",
    "\n",
    "# Referências\n",
    "\n",
    "- Documentação SciKit-Learn:\n",
    "  - https://scikit-learn.org/stable/documentation.html\n",
    "- Exemplo completo interessante:\n",
    "  * https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
